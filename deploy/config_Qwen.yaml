# config.yaml

model: Qwen/Qwen3-0.6B
host: "0.0.0.0"
port: 8000
uvicorn-log-level: "info"
dtype: "float16"  # Explicitly set to minimize memory
gpu-memory-utilization: 0.1  # Reduced to fit available VRAM
enable-reasoning: true
reasoning-parser: "deepseek_r1"
api-key: "focus-deploy"
max-model-len: 1024
max-num-batched-tokens: 2048
max-num-seqs: 64