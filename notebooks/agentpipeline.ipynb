{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,Literal\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel,Field\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21df93",
   "metadata": {},
   "source": [
    "# Basic Agents function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc15a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification , BitsAndBytesConfig\n",
    "# Load tokenizer and model\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_name = \"/home/siamai/data/Focus/agentic/notebooks/model/xlm_routing\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_classifier = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                           num_labels=2,\n",
    "                                                           quantization_config = quantization_config,\n",
    "                                                           device_map=\"auto\",\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec392d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Alternative memory-efficient loading options without bitsandbytes\n",
    "\n",
    "model_id = \"/home/siamai/data/huggingface/hub/models--tarun7r--Finance-Llama-8B/snapshots/7934db35d2374c1321b90a9deb0d84b97525b025\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_multiple = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",         \n",
    "    low_cpu_mem_usage=True,     \n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_multiple,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72894710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_xlm(text:str):\n",
    "    dict = {0:\"multiple\",1:\"prediction\"}\n",
    "    inputs = tokenizer(text, \n",
    "                       padding=True, \n",
    "                       truncation=True, \n",
    "                       max_length=512,\n",
    "                       return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model_classifier(**inputs)\n",
    "    logits = outputs.logits.argmax(dim=1)\n",
    "    return dict[logits.item()]\n",
    "\n",
    "def prediction_answer(text:str):\n",
    "    pass\n",
    "\n",
    "def multiple_answer(text:str,system_prompt:str):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": text}]\n",
    "    prompt = \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in messages])\n",
    "    outputs = generator(\n",
    "                prompt,\n",
    "                max_new_tokens=32,         # Reduced for memory efficiency\n",
    "                do_sample=True,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                # Memory efficient generation settings\n",
    "                num_beams=3,                # No beam search to save memory\n",
    "                early_stopping=True,\n",
    "                use_cache=True\n",
    "                )\n",
    "    # Extract response\n",
    "    response = outputs[0]['generated_text'].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe34f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a highly knowledgeable finance chatbot. Your purpose is to select answer choice from user query you can select only character that represent the answer follwing by A, B, C, D\"\n",
    "text = \"\"\"Answer the question with the appropriate options A, B, C and D. Please respond with the exact answer A, B, C or D only. Do not be verbose or provide extra information. \n",
    "Question: According to COSO, which of the following is the most effective method to transmit a message of ethical behavior throughout an organization?\n",
    "Answer Choices: A: Demonstrating appropriate behavior by example., B: Strengthening internal auditâ€™s ability to deter and report improper behavior., C: Removing pressures to meet unrealistic targets, particularly for short-term results., D: Specifying the competence levels for every job in an organization and translating those levels to requisite knowledge and skills. \n",
    "Answer:\"\"\"\n",
    "multiple_answer(text = text,\n",
    "               system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30f2dd",
   "metadata": {},
   "source": [
    "# Function with Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c399ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_MULTIPLE =\"\"\"You are a highly knowledgeable finance chatbot. Your purpose is to select answer choice from user query you can select only character that represent the answer follwing by A, B, C, D\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277418bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageClassifier(BaseModel):\n",
    "    message_type: Literal[\"multiple\",\"prediction\"] = Field(\n",
    "        ...,\n",
    "        description=\"Classify if the message is multiple or prediction\",\n",
    "    )\n",
    "\n",
    "class State(TypedDict):\n",
    "    message: Annotated[list,add_messages]\n",
    "    message_type: str | None\n",
    "    next: str\n",
    "\n",
    "def classify_message(state: State) -> State:\n",
    "    messsage = state[\"message\"][-1].content\n",
    "    message_type = classify_xlm(messsage)\n",
    "    validated_type = MessageClassifier(message_type=message_type) \n",
    "    return {\"message_type\":validated_type.message_type}\n",
    "\n",
    "def router(state: State) -> State:\n",
    "    message_type = state.get(\"message_type\")\n",
    "    return {\"next\":message_type}\n",
    "\n",
    "def prediction_agent(state: State) -> State:\n",
    "    message = state[\"message\"][-1].content\n",
    "    message = f\"Hello User I'm a prediction_agent agent! JUST PLACE HOLDER\"\n",
    "    return {\"message\":message}\n",
    "\n",
    "def multiple_agent(state: State) -> State:\n",
    "    message = state[\"message\"][-1].content\n",
    "    respond = multiple_answer(text = message,system_prompt=PROMPT_MULTIPLE)\n",
    "    return {\"message\":respond}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"classifier\",classify_message)\n",
    "graph_builder.add_node(\"router\",router)\n",
    "graph_builder.add_node(\"prediction_agent\",prediction_agent)\n",
    "graph_builder.add_node(\"multiple_agent\",multiple_agent)\n",
    "\n",
    "graph_builder.add_edge(START,\"classifier\")\n",
    "graph_builder.add_edge(\"classifier\",\"router\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state.get(\"next\"),\n",
    "    {\n",
    "        \"prediction\": \"prediction_agent\",\n",
    "        \"multiple\": \"multiple_agent\"\n",
    "    }\n",
    ")\n",
    "graph_builder.add_edge(\"prediction_agent\",END)\n",
    "graph_builder.add_edge(\"multiple_agent\",END)\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly select row from dataframe as input\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/siamai/data/Focus/agentic/data/test.csv\")\n",
    "user_input = df.sample(n=1).iloc[0][\"query\"]\n",
    "print(f\"User input: {user_input}\")\n",
    "state = graph.invoke({\"message\":[user_input]})     \n",
    "state[\"message\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png(max_retries=10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
