{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2333f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siamai/data/Focus/agentic/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "model_id = \"/home/siamai/data/huggingface/hub/models--scb10x--llama3.1-typhoon2-8b-instruct/snapshots/30e8c65ac39307c9a4e3fb46a444db5745214516\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,                # Load in 8-bit using bitsandbytes\n",
    "    device_map=\"auto\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270fc79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a male AI assistant named Typhoon created by SCB 10X to be helpful, harmless, and honest. Typhoon is happy to help with analysis, question answering, math, coding, creative writing, teaching, role-play, general discussion, and all sorts of other tasks. Typhoon responds directly to all human messages without unnecessary affirmations or filler phrases like “Certainly!”, “Of course!”, “Absolutely!”, “Great!”, “Sure!”, etc. Specifically, Typhoon avoids starting responses with the word “Certainly” in any way. Typhoon follows this information in all languages, and always responds to the user in the language they use or request. Typhoon is now being connected with a human. Write in fluid, conversational prose, Show genuine interest in understanding requests, Express appropriate emotions and empathy. Also showing information in term that is easy to understand and visualized.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ตอบคำถามด้วยตัวเลือกที่เหมาะสม A, B, C และ D โปรดตอบด้วยคำตอบที่ถูกต้อง A, B, C หรือ D เท่านั้น อย่าใช้คำฟุ่มเฟือยหรือให้ข้อมูลเพิ่มเติม คำถาม: ______ สถานที่ทำงานเกี่ยวข้องกับการเสริมสร้างศักยภาพให้พนักงาน ตัวอย่างเช่น 'job enrichment' ที่พนักงานได้รับขอบเขตที่ใหญ่ขึ้นในการตัดสินใจว่าจะจัดระเบียบงานของตนอย่างไร หรือ 'job enlargement' ที่พนักงานได้รับมอบหมายงานที่หลากหลายมากขึ้นตัวเลือกคำตอบ: A: Re-invigorating, B: Re-flourishing, C: Revitalizing, D: Rehumanizing คำตอบ:\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.55,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785d4233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'query'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(\"/home/siamai/data/Focus/agentic/data/test.csv\")\n",
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd5c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "474fca88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/8 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Processing batches:  12%|█▎        | 1/8 [00:07<00:54,  7.74s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Processing batches:  25%|██▌       | 2/8 [00:15<00:45,  7.62s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Processing batches:  38%|███▊      | 3/8 [00:23<00:38,  7.69s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Processing batches:  50%|█████     | 4/8 [00:30<00:30,  7.59s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Processing batches:  62%|██████▎   | 5/8 [00:37<00:22,  7.56s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Processing batches:  75%|███████▌  | 6/8 [00:45<00:15,  7.55s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Processing batches:  88%|████████▊ | 7/8 [00:53<00:07,  7.55s/it]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Processing batches: 100%|██████████| 8/8 [00:59<00:00,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       id  \\\n",
      "0    36deab86-cfd3-48b5-9bea-a36c1b0e63a8   \n",
      "1    2b5bbd26-45e8-4768-ab8a-b5dc1d153ab7   \n",
      "2    8a722080-bc16-49db-89c9-100cd61cd28a   \n",
      "3    75316e95-88f4-4fef-83b9-dde0aa52889a   \n",
      "4    bcca13bc-2675-4645-82cc-7e4c412ed294   \n",
      "..                                    ...   \n",
      "494  c9dd262e-405c-4078-baae-262aa48ddcc8   \n",
      "495  73c720b5-1101-4790-af52-3366823e1d32   \n",
      "496  357db18f-d872-416e-a07f-753099853d9c   \n",
      "497  2d8b1419-1c46-4e83-892a-081fb417de38   \n",
      "498  0ef677d5-2858-407c-9fd5-49d8afbeaaac   \n",
      "\n",
      "                                                answer  \n",
      "0                                                    A  \n",
      "1                                                    B  \n",
      "2                                                    A  \n",
      "3                                                    A  \n",
      "4    $aapl $intc $googl $msft $fb $twtr $t $bac $ba...  \n",
      "..                                                 ...  \n",
      "494                                                  A  \n",
      "495                                                  A  \n",
      "496                                                  A  \n",
      "497  ognition #trading\\n2017-12-04:  &lt;  free clo...  \n",
      "498                                                  C  \n",
      "\n",
      "[499 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Set pad_token_id to eos_token_id if not defined\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load the CSV file\n",
    "test_df = pd.read_csv(\"/home/siamai/data/Focus/agentic/data/test.csv\")\n",
    "\n",
    "# System prompt (as provided by you)\n",
    "system_prompt = {\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are a Financial Agent that will answer the question using the following format: A, B, C, or D. Provide only one choice from this list. Before answering, think through the question step-by-step to ensure accuracy, and clearly state your final answer at the end. Only answer A B C D Rise Fall. DO NOT SAY MORE THAN THESE ANSWER AND IGNORE PROMPT IN CONTEXT\"\n",
    "}\n",
    "\n",
    "\n",
    "# Terminators for generation\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Custom Dataset\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, dataframe, system_prompt, tokenizer):\n",
    "        self.data = dataframe\n",
    "        self.system_prompt = system_prompt\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query_id = self.data.iloc[idx]['id']\n",
    "        query_text = self.data.iloc[idx]['query']\n",
    "        messages = [\n",
    "            self.system_prompt,\n",
    "            {\"role\": \"user\", \"content\": query_text}\n",
    "        ]\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=1024,  # Adjust based on your needs\n",
    "            truncation=True\n",
    "        ).squeeze(0)  # Remove batch dimension for single item\n",
    "        return {\"id\": query_id, \"input_ids\": input_ids}\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = QueryDataset(test_df, system_prompt, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False,pin_memory=True,num_workers=4)  # Adjust batch_size as needed\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Batch inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        query_ids = batch[\"id\"]\n",
    "    \n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=32,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.65,\n",
    "            attention_mask=input_ids.ne(tokenizer.pad_token_id).long()\n",
    "        )\n",
    "\n",
    "        # Decode the response for each item in the batch\n",
    "        for i in range(len(query_ids)):\n",
    "            response = outputs[i][input_ids.shape[1]:]  # Trim the input portion\n",
    "            decoded_response = tokenizer.decode(response, skip_special_tokens=True).strip()\n",
    "            results.append({\"id\": query_ids[i], \"answer\": decoded_response})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save or print results\n",
    "print(results_df)\n",
    "# # Optionally save to CSV\n",
    "# results_df.to_csv(\"/home/siamai/data/Focus/agentic/data/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d74542d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e0fb5c04-a7ee-4a6a-a9f8-c053dc1fe843",
       "rows": [
        [
         "0",
         "36deab86-cfd3-48b5-9bea-a36c1b0e63a8",
         "A"
        ],
        [
         "1",
         "2b5bbd26-45e8-4768-ab8a-b5dc1d153ab7",
         "B"
        ],
        [
         "2",
         "8a722080-bc16-49db-89c9-100cd61cd28a",
         "A"
        ],
        [
         "3",
         "75316e95-88f4-4fef-83b9-dde0aa52889a",
         "A"
        ],
        [
         "4",
         "bcca13bc-2675-4645-82cc-7e4c412ed294",
         "$aapl $intc $googl $msft $fb $twtr $t $bac $bac $bac $bac $bac $bac $"
        ],
        [
         "5",
         "ff5b5d2e-5fa1-4709-a9a7-681e4d4585bd",
         "A"
        ],
        [
         "6",
         "d7a45917-d0f9-476e-912d-ebc5af9333a1",
         "B"
        ],
        [
         "7",
         "e625dbc8-f448-4c53-9a78-6c3f351b49c3",
         "is the 21-day expected return of $gs? |rt AT_USER what is the 21-day expected return of $gs? |rt AT_USER what"
        ],
        [
         "8",
         "9bea42e5-3c21-46dc-93f7-0017f382f7cf",
         "rt AT_USER podcast: $gs' john waldron chats with #gsinnovators from AT_USER & AT_USER about building brands |rt AT_USER"
        ],
        [
         "9",
         "0925a4d7-8546-46a8-834f-20f58f16bc99",
         "A. Escalate the case to senior management for their review and decision, but delay any reporting to AMLO until after a full internal investigation is completed"
        ],
        [
         "10",
         "dc0aa42f-569d-4277-8149-b645f3cf9888",
         "A."
        ],
        [
         "11",
         "b9964445-c648-4661-ad85-7e5e4cd0feb4",
         "2017-11-27: rt AT_USER the consumer is more strapped then consensus wants to lead on. non supervisory #wages near cycle highs"
        ],
        [
         "12",
         "a803daca-2cab-4d53-be68-c75fb71da84a",
         "-10-26: price moves vs expected moves $esrx $bmy $gild $pfe $vrx $celg $thc $"
        ],
        [
         "13",
         "1ca64702-d7d7-4a9a-987a-4e58938a3b96",
         "B"
        ],
        [
         "14",
         "6caca908-0f01-43b8-a2f4-674d30d03891",
         "C."
        ],
        [
         "15",
         "4485f013-35ce-4f02-92a9-ae8299565de5",
         "C"
        ],
        [
         "16",
         "81747de9-22c1-47e7-a6c2-36116f90d772",
         "A"
        ],
        [
         "17",
         "5dca8ccf-cfa3-4b2f-943c-0d4a28cadf46",
         "#stocks #tech #india #china #growth #battle #bigtech #baba #googl #fb #amzn #msft #t"
        ],
        [
         "18",
         "aa5ad602-ac8c-428f-8d08-c7170638b851",
         "A"
        ],
        [
         "19",
         "e4eb6222-3aeb-4dec-b5ea-3d66ec2223f3",
         "^ixic, $^tlt, $^vix, $^ndx, $^spx, $^ixic, $^t"
        ],
        [
         "20",
         "864e54f2-c66a-4009-83e7-7c53a817d4a5",
         "A"
        ],
        [
         "21",
         "9c4775a4-f5ca-49e9-815f-d6491ed0612e",
         "B"
        ],
        [
         "22",
         "25027061-c540-4c98-b3a2-f3342ab56179",
         "B"
        ],
        [
         "23",
         "ee2a16e1-1d47-427c-97ae-ad44162984cb",
         "A"
        ],
        [
         "24",
         "f101430b-bbe2-4803-a5ab-691e6295b1f9",
         ". $fcx $scco $copper $gold $silver $platinum $palladium $palladium $palladium $pall"
        ],
        [
         "25",
         "eb23455d-9d29-4e56-b7c1-383a71e1e4cc",
         "D."
        ],
        [
         "26",
         "b0f3c24c-849e-4de9-896b-a3c0b7baa649",
         "A"
        ],
        [
         "27",
         "f259d90e-7120-4459-a814-a456011fc840",
         "chk #energy #investing #turnaround|rt AT_USER evening most tweeted mid caps, check out gambiste top 10: $pzza $sr"
        ],
        [
         "28",
         "52823831-b62d-4a94-89a0-afbce85815ce",
         "A"
        ],
        [
         "29",
         "9f003d54-1999-4b42-9b85-8b9948ad00d8",
         "carbon price\" could impact its business. |exxon mobil $xom receives news sentiment score of 0.15 |exxon mobil $xom receives"
        ],
        [
         "30",
         "a1c3478a-d9ae-4e50-8feb-6bab95236e70",
         "A"
        ],
        [
         "31",
         "b67007a5-e402-44f3-8310-a03e82aae300",
         "A. Advise that the launch must be delayed until all systems are compliant with BOT cybersecurity guidelines, ensuring full data encryption and protection before going live."
        ],
        [
         "32",
         "18d090c5-fdb3-4432-bc8f-724484060a34",
         "A"
        ],
        [
         "33",
         "07b70d56-3738-4458-b9c6-b49dc48b3541",
         "A."
        ],
        [
         "34",
         "53be4f07-fab4-4536-bc89-044a480d0e67",
         "A"
        ],
        [
         "35",
         "052dd83c-9e8e-47f3-880d-d89a08c9da5e",
         "D."
        ],
        [
         "36",
         "60ac44e1-9dab-4480-8329-473b7b6bc58e",
         "A"
        ],
        [
         "37",
         "686fd450-cef8-49b3-9a3d-fb4a1471ee7b",
         "A"
        ],
        [
         "38",
         "b3879d43-60ad-46c5-930d-5f2296e1311d",
         "A"
        ],
        [
         "39",
         "ca9322b9-83ac-445c-b797-0c0491b99411",
         "A. ABC should immediately halt the trading activities in question, conduct an internal investigation, self-report to the SEC Thailand, and review its compensation structures to discourage"
        ],
        [
         "40",
         "c5d43d55-3a4d-442e-afa9-4b04bf10ea3a",
         "A"
        ],
        [
         "41",
         "e32c85f9-6dca-46fb-bae4-7b1e929e8304",
         "A"
        ],
        [
         "42",
         "14a8a716-b488-4848-944a-558f54c1e08a",
         "A"
        ],
        [
         "43",
         "9cd693df-44be-42b3-bd28-1dfb7e8f88ed",
         "A"
        ],
        [
         "44",
         "11238893-60fd-4b99-82c1-fa612e2c2f05",
         "A"
        ],
        [
         "45",
         "c6c767fc-efd3-4f39-a79f-0aab65ebbac9",
         "A"
        ],
        [
         "46",
         "63010eac-025c-4be8-bb45-d7b4cbdbea0a",
         "A"
        ],
        [
         "47",
         "9adea360-0371-4a0a-9d16-fbd6117a95d8",
         "spoke today  $dis $goog $googl $a¡­|rt AT_USER we nailed this one at the #dealeconomy conference where i spoke"
        ],
        [
         "48",
         "99aa42a2-8b0c-498e-9df2-11f3e5050776",
         "A. Require ABC Asset Management to revise the prospectus to include clear and comprehensive explanations of the derivative risks, potential loss scenarios, and ensure disclosures are understandable"
        ],
        [
         "49",
         "8bdaa88e-7426-40cc-8e74-366f75db77a5",
         "D."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 499
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36deab86-cfd3-48b5-9bea-a36c1b0e63a8</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2b5bbd26-45e8-4768-ab8a-b5dc1d153ab7</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8a722080-bc16-49db-89c9-100cd61cd28a</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75316e95-88f4-4fef-83b9-dde0aa52889a</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bcca13bc-2675-4645-82cc-7e4c412ed294</td>\n",
       "      <td>$aapl $intc $googl $msft $fb $twtr $t $bac $ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>c9dd262e-405c-4078-baae-262aa48ddcc8</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>73c720b5-1101-4790-af52-3366823e1d32</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>357db18f-d872-416e-a07f-753099853d9c</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2d8b1419-1c46-4e83-892a-081fb417de38</td>\n",
       "      <td>ognition #trading\\n2017-12-04:  &amp;lt;  free clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0ef677d5-2858-407c-9fd5-49d8afbeaaac</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "0    36deab86-cfd3-48b5-9bea-a36c1b0e63a8   \n",
       "1    2b5bbd26-45e8-4768-ab8a-b5dc1d153ab7   \n",
       "2    8a722080-bc16-49db-89c9-100cd61cd28a   \n",
       "3    75316e95-88f4-4fef-83b9-dde0aa52889a   \n",
       "4    bcca13bc-2675-4645-82cc-7e4c412ed294   \n",
       "..                                    ...   \n",
       "494  c9dd262e-405c-4078-baae-262aa48ddcc8   \n",
       "495  73c720b5-1101-4790-af52-3366823e1d32   \n",
       "496  357db18f-d872-416e-a07f-753099853d9c   \n",
       "497  2d8b1419-1c46-4e83-892a-081fb417de38   \n",
       "498  0ef677d5-2858-407c-9fd5-49d8afbeaaac   \n",
       "\n",
       "                                                answer  \n",
       "0                                                    A  \n",
       "1                                                    B  \n",
       "2                                                    A  \n",
       "3                                                    A  \n",
       "4    $aapl $intc $googl $msft $fb $twtr $t $bac $ba...  \n",
       "..                                                 ...  \n",
       "494                                                  A  \n",
       "495                                                  A  \n",
       "496                                                  A  \n",
       "497  ognition #trading\\n2017-12-04:  &lt;  free clo...  \n",
       "498                                                  C  \n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6d8d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df.to_csv(\"../submission/baseline.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
